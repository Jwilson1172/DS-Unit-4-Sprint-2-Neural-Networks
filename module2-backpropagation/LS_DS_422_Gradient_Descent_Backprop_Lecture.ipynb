{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Backpropagation & Gradient Descent (Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Explain the intutition behind backproprogation\n",
    "* <a href=\"#p2\">Part 2</a>: Implement gradient descent + backpropagation on a feedforward neural network. \n",
    "* <a href=\"#p3\">Part 3</a>: Introduce the Keras Sequential Model API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Yesterday\n",
    "\n",
    "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinforce our understanding of those components and introduce the mechanics of training a neural network. Feed-forward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*_M4bZyuwaGby6KMiYVYXvg.jpeg\" width=\"400\"></center>\n",
    "\n",
    "- There are three kinds of layers: input, hidden, and output layers.\n",
    "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
    "- Signal is passed from layer to layer through a network by:\n",
    " - Taking in inputs from the training data (or previous layer)\n",
    " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
    " - Adding a bias to this weighted some of inputs and weights\n",
    " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network: *Formal Summary*\n",
    "\n",
    "0. Pick a network architecture\n",
    "   - No. of input units = No. of features\n",
    "   - No. of output units = Number of Classes (or expected targets)\n",
    "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement code to compute a cost function $J(\\theta)$\n",
    "4. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
    "5. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
    "6. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Calculating *\"cost\"*, *\"loss\"* or *\"error\"*\n",
    "\n",
    "We've talked about how in order to evaluate a network's performance, the data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it *should* have predicted. \n",
    "\n",
    "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value. \n",
    "\n",
    "We can summarize the overall quality of a network's predictions by finding the average error across all observations. This gives us the \"Mean Squared Error.\" which hopefully is a fairly familiar model evaluation metric by now. Graphing the MSE over each epoch (training cycle) is a common practice with Neural Networks. This is what you're seeing in the top right corner of the Tensorflow Playground website as the number of \"epochs\" climbs higher and higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss functions \n",
    "- mse\n",
    "- rmse\n",
    "- accuracy ect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the backprop to compute the partial dervivitives\n",
    "\n",
    "# the simple intuition of the derivitive is to find what direction the inputs of the activation need to move in order to maximize the activation to effect the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an \"Epoch\"?\n",
    "\n",
    "An \"Epoch\" is one cycle of passing our data forward through the network, measuring error given our specified cost function, and then -via gradient descent- updating weights within our network to hopefully improve the quality of our predictions on the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about Hyperparameters\n",
    "\n",
    "Neural Networks have many more hyperparameters than other machine learning algorithms which is part of what makes them a beast to train.\n",
    "\n",
    "1. You need more data to train them on. \n",
    "2. They're complex so they take longer to train. \n",
    "3. They have lots and lots of hyperparameters which we need to find the most optimal combination of, so we might end up training our model dozens or hundreds of times with different combinations of hyperparameters in order to try and squeeze out a few more tenths of a percent of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Backpropagation (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walk through of the main intuitions and math behind the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Gradient?\n",
    "\n",
    "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
    "\n",
    "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "In this section, we will again implement a multi-layer perceptron using numpy. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dm2HPETcrgy6",
    "toc-hr-collapsed": true
   },
   "source": [
    "![231 Neural Network](https://cdn-images-1.medium.com/max/1600/1*IjY3wFF24sK9UhiOlf36Bw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4d4tzpwO6B47"
   },
   "source": [
    "### Generate some Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERyVgeO_IWyV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "# Imagine that our data is drawn from a linear function\n",
    "# y = 2*hours_studying + 4*hours_sleeping + 50\n",
    "\n",
    "# hours studying, hours sleep\n",
    "X = np.array(([2,9],\n",
    "              [1,5],\n",
    "              [3,6]), dtype=float)\n",
    "# 2 hours studing and 9 hours sleeping\n",
    "# 1 hour sutuding 5 hours sleeping\n",
    "# 3 hours studing 6 ours sleeping\n",
    "\n",
    "# Exam Scores\n",
    "y = np.array(([90],\n",
    "              [72],\n",
    "              [80]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x\n [[2. 9.]\n [1. 5.]\n [3. 6.]]\ny\n [[90.]\n [72.]\n [80.]]\n"
    }
   ],
   "source": [
    "print(\"x\\n\",X)\n",
    "print(\"y\\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDeUBW6k4Ri4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Studying, Sleeping \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nTest Score \n [[0.9 ]\n [0.72]\n [0.8 ]]\n"
    }
   ],
   "source": [
    "# Normalizing Data on feature \n",
    "# Neural Network would probably do this on its own, but it will help us converge on a solution faster\n",
    "X = X / np.amax(X, axis=0)\n",
    "\n",
    "# essentially already normailized just going to make it a float\n",
    "y = y / 100\n",
    "\n",
    "print(\"Studying, Sleeping \\n\", X)\n",
    "print(\"Test Score \\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgTf6vTS69Sw"
   },
   "source": [
    "### Neural Network Architecture\n",
    "Lets create a Neural_Network class to contain this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUI8VSR5zyBv"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    # there are a bunch of ways to set up an arch this is just one way\n",
    "    def __init__(self):\n",
    "        # Set up Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        self.weights1 = np.random.randn(self.inputs,self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbyT_FJ88IlK"
   },
   "source": [
    "### Randomly Initialize Weights\n",
    "How many random weights do we need to initialize? \"Fully-connected Layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IreIDe6P8H0H"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Layer 1 weights: \n [[ 2.48783189  0.11697987 -1.97118428]\n [-0.48325593 -1.50361209  0.57515126]]\nLayer 2 weights: \n [[-0.20672583]\n [ 0.41271104]\n [-0.57757999]]\n"
    }
   ],
   "source": [
    "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
    "print(\"Layer 2 weights: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these are the things that we are tring to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbxDhyjQ-RwS"
   },
   "source": [
    "### Implement Feedforward Functionality\n",
    "\n",
    "After this step our neural network should be able to generate an output even though it has not been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gGivpEk-VdP"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        # Input to Hidden (1st set of weights)\n",
    "        self.weights1 = np.random.randn(self.inputs,self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        # Hidden to Output (2nd set of weights)\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        # taking the dot products of the inputs and the weights of the hidden layer\n",
    "        \n",
    "        # Weighted Sum\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activate\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        # we are ignoring the bias term today for simplicity\n",
    "        \n",
    "        # Weighted sum of activated hidden (which output layer will use)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final Activation of Output (My Predictions)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.66666667, 1.        ],\n       [0.33333333, 0.55555556],\n       [1.        , 0.66666667]])"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1pxdfmDAaJg"
   },
   "source": [
    "### Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.66666667 1.        ]\noutput [0.25814933]\n"
    }
   ],
   "source": [
    "# Try to make a prediction with our updated 'net\n",
    "nn = NeuralNetwork()\n",
    "print(X[0])\n",
    "output = nn.feed_forward(X[0])\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V61yNmAB2T5"
   },
   "source": [
    "### Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.64185067])"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "error = y[0] - output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.25814933]\n [0.33067192]\n [0.22642076]]\n[[0.64185067]\n [0.38932808]\n [0.57357924]]\n"
    }
   ],
   "source": [
    "output_all = nn.feed_forward(X)\n",
    "# rember that on an absolute loss binary classification is only good for just that\n",
    "# as the complexity of the problem goes up so does the complexity of the loss function\n",
    "error_all = y - output_all\n",
    "print(output_all)\n",
    "print(error_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26wgCLU0TLvy"
   },
   "source": [
    "Why is my error so big?\n",
    "\n",
    "My error is so big because my prediction is low.\n",
    "\n",
    "Why are my prediction low?\n",
    "\n",
    "Because either:\n",
    "\n",
    "  1) Second layer **weights** are low\n",
    "  \n",
    "  (or)\n",
    "  \n",
    "  2) Activations coming from the first layer are low\n",
    "  \n",
    "How are activations from the first layer determined? \n",
    "\n",
    "  1) By inputs - fixed\n",
    "  \n",
    "  2) by **weights** - variable\n",
    "  \n",
    "The only thing that I have control over throughout this process in order to increase the value of my final predictions is to either increase weights in layer 2 or increase weights in layer 1. \n",
    "\n",
    "Imagine that you could only change your weights by a fixed amount. Say you have .3 and you have to split that up and disperse it over your weights so as to increase your predictions as much as possible. (This isn't actually what happens, but it will help us identify which weights we would benefit the most from moving.)\n",
    "\n",
    "I need to increase weights of my model somewhere, I'll get the biggest bang for my buck if I increase weights in places where I'm already seeing high activation values -because they end up getting multiplied together before being passed to the sigmoid function. \n",
    "\n",
    "> \"Neurons that fire together, wire together\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_eyzItYIxgm"
   },
   "source": [
    "### Implement Backpropagation \n",
    "ean goodfellow's book\n",
    "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
    "\n",
    "What in our model could be causing our predictions to suck so bad? \n",
    "\n",
    "Well, we know that our inputs (X) and outputs (y) are correct, if they weren't then we would have bigger problems than understanding backpropagation.\n",
    "\n",
    "We also know that our activation function (sigmoid) is working correctly. It can't be blamed because it just does whatever we tell it to and transforms the data in a known way.\n",
    "\n",
    "So what are the potential culprits for these terrible predictions? The **weights** of our model. Here's the problem though. I have weights that exist in both layers of my model. How do I know if the weights in the first layer are to blame, or the second layer, or both? \n",
    "\n",
    "Lets investigate. And see if we can just eyeball what should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "weights1\n [[-1.75351135  1.23279898  0.24464757]\n [-0.06568225  0.30190098  0.79723428]] \n---------\nhidden_sum\n [[-1.23468981  1.12376697  0.96033266]\n [-0.62099392  0.57865576  0.52445712]\n [-1.79729952  1.4340663   0.77613709]] \n---------\nactivated_hidden\n [[0.22536165 0.75468678 0.7231884 ]\n [0.34955543 0.64075804 0.6281894 ]\n [0.14218011 0.8075341  0.68484697]] \n---------\nweights2\n [[ 1.23073545]\n [-1.52187331]\n [-0.25502715]] \n---------\nactivated_output\n [[0.25814933]\n [0.33067192]\n [0.22642076]] \n---------\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[None, None, None, None, None]"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'activated_output']\n",
    "[print(i+'\\n', getattr(nn,i), '\\n'+'---'*3) for i in attributes if i[:2]!= '__'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Update Weights Based on Gradient\n",
    "\n",
    "Repeat steps 2-4 for every observation in a given batch, and then given the network's cost function, calculate its gradient using calculus and update weights associated with the (negative) gradient of the cost function. \n",
    "\n",
    "Remember that we have 9 weights in our network therefore the gradient that comes from our gradient descent calculation will be the vector that takes us in the most downward direction along some function in 9-dimensional hyperspace.\n",
    "\n",
    "\\begin{align}\n",
    "C(w1, w2, w3, w4, w5, w6, w7, w8, w9)\n",
    "\\end{align}\n",
    "\n",
    "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
    "\n",
    "1) Stochastic Gradient Descent\n",
    "\n",
    "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        # Input to Hidden (1st set of weights)\n",
    "        self.weights1 = np.random.randn(self.inputs,self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        # Hidden to Output (2nd set of weights)\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted Sum\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activate\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weighted sum of activated hidden (which output layer will use)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final Activation of Output (My Predictions)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Back prop thru the network\n",
    "        \"\"\"\n",
    "        # with respect to error looking at the inputs and activation function if you\n",
    "        # multiple the activation by the derrivite you can boost the weights to improve the prediction\n",
    "\n",
    "        # true vinella gradient decent\n",
    "        # more complex error landscapes require more complex activation functions\n",
    "\n",
    "        # Error in the output\n",
    "        self.o_error = y - o \n",
    "        \n",
    "        # Apply derivative of sigmoid to error\n",
    "        # gradient of the output layer, what went wrong\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error: how much were our output layer weights off\n",
    "        # how much of that error is from the neruons in the previous layer\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # z2 delta: how much were the weights off?\n",
    "        # from those neruons how off were the weights\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.output_sum)\n",
    "\n",
    "        # now that we know how far the weights were off we can bump the weights to improve the results in the output layer.1\n",
    "        #Adjust first set (input => hidden) weights\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "\n",
    "        #adjust second set (hidden => output) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) \n",
    "        \n",
    "    def train(self, X,y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Let's look at the shape of the Gradient Componets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Error Associated with Each Observation \n",
    "aka how wrong were we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.60746115],\n       [0.46613403],\n       [0.53667427]])"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "nn.o_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Gradient \n",
    "Simple interpretation - how much more sigmoid activation would have pushed us towards the right answer?\n",
    "\n",
    "`self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.12602312],\n       [0.07311531],\n       [0.09520125]])"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "nn.o_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the derivate of the sigmoid function to understand what's happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "line_x = np.arange(-5, 5, 0.01)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "# sigmoid\n",
    "y = sigmoid(line_x)\n",
    "y_d = sigmoid_derivative(line_x)\n",
    "\n",
    "x = nn.output_sum\n",
    "s = sigmoid(x)\n",
    "sx = sigmoid_derivative(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1080x360 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"313.718125pt\" version=\"1.1\" viewBox=\"0 0 868.8 313.718125\" width=\"868.8pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 313.718125 \nL 868.8 313.718125 \nL 868.8 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 24.6 293.69625 \nL 405.054545 293.69625 \nL 405.054545 21.89625 \nL 24.6 21.89625 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 76.514787 293.69625 \nL 76.514787 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\"/>\n     <g id=\"text_1\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 4.9375 29.6875 \nL 4.9375 36.8125 \nL 53.515625 36.8125 \nL 53.515625 29.6875 \nz\n\" id=\"LiberationSans-8722\"/>\n       <path d=\"M 43.015625 15.578125 \nL 43.015625 0 \nL 34.71875 0 \nL 34.71875 15.578125 \nL 2.296875 15.578125 \nL 2.296875 22.40625 \nL 33.796875 68.796875 \nL 43.015625 68.796875 \nL 43.015625 22.515625 \nL 52.6875 22.515625 \nL 52.6875 15.578125 \nz\nM 34.71875 58.890625 \nQ 34.625 58.59375 33.34375 56.296875 \nQ 32.078125 54 31.453125 53.078125 \nL 13.8125 27.09375 \nL 11.1875 23.484375 \nL 10.40625 22.515625 \nL 34.71875 22.515625 \nz\n\" id=\"LiberationSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(70.814005 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-8722\"/>\n       <use x=\"58.398438\" xlink:href=\"#LiberationSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 145.757583 293.69625 \nL 145.757583 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\"/>\n     <g id=\"text_2\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 5.03125 0 \nL 5.03125 6.203125 \nQ 7.515625 11.921875 11.109375 16.28125 \nQ 14.703125 20.65625 18.65625 24.1875 \nQ 22.609375 27.734375 26.484375 30.765625 \nQ 30.375 33.796875 33.5 36.8125 \nQ 36.625 39.84375 38.546875 43.15625 \nQ 40.484375 46.484375 40.484375 50.6875 \nQ 40.484375 56.34375 37.15625 59.46875 \nQ 33.84375 62.59375 27.9375 62.59375 \nQ 22.3125 62.59375 18.671875 59.546875 \nQ 15.046875 56.5 14.40625 50.984375 \nL 5.421875 51.8125 \nQ 6.390625 60.0625 12.421875 64.9375 \nQ 18.453125 69.828125 27.9375 69.828125 \nQ 38.328125 69.828125 43.921875 64.921875 \nQ 49.515625 60.015625 49.515625 50.984375 \nQ 49.515625 46.96875 47.671875 43.015625 \nQ 45.84375 39.0625 42.234375 35.109375 \nQ 38.625 31.15625 28.421875 22.859375 \nQ 22.796875 18.265625 19.46875 14.578125 \nQ 16.15625 10.890625 14.703125 7.46875 \nL 50.59375 7.46875 \nL 50.59375 0 \nz\n\" id=\"LiberationSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(140.056802 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-8722\"/>\n       <use x=\"58.398438\" xlink:href=\"#LiberationSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 215.00038 293.69625 \nL 215.00038 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\"/>\n     <g id=\"text_3\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 51.703125 34.421875 \nQ 51.703125 17.1875 45.625 8.09375 \nQ 39.546875 -0.984375 27.6875 -0.984375 \nQ 15.828125 -0.984375 9.859375 8.046875 \nQ 3.90625 17.09375 3.90625 34.421875 \nQ 3.90625 52.15625 9.6875 60.984375 \nQ 15.484375 69.828125 27.984375 69.828125 \nQ 40.140625 69.828125 45.921875 60.890625 \nQ 51.703125 51.953125 51.703125 34.421875 \nz\nM 42.78125 34.421875 \nQ 42.78125 49.3125 39.328125 56 \nQ 35.890625 62.703125 27.984375 62.703125 \nQ 19.875 62.703125 16.328125 56.109375 \nQ 12.796875 49.515625 12.796875 34.421875 \nQ 12.796875 19.78125 16.375 12.984375 \nQ 19.96875 6.203125 27.78125 6.203125 \nQ 35.546875 6.203125 39.15625 13.125 \nQ 42.78125 20.0625 42.78125 34.421875 \nz\n\" id=\"LiberationSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(212.219911 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 284.243176 293.69625 \nL 284.243176 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\"/>\n     <g id=\"text_4\">\n      <!-- 2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(281.462707 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 353.485973 293.69625 \nL 353.485973 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\"/>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(350.705504 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 24.6 283.006703 \nL 405.054545 283.006703 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\"/>\n     <g id=\"text_6\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 9.125 0 \nL 9.125 10.6875 \nL 18.65625 10.6875 \nL 18.65625 0 \nz\n\" id=\"LiberationSans-46\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 286.63014)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-48\"/>\n       <use x=\"55.615234\" xlink:href=\"#LiberationSans-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#LiberationSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 24.6 232.916931 \nL 405.054545 232.916931 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\"/>\n     <g id=\"text_7\">\n      <!-- 0.2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 236.540369)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-48\"/>\n       <use x=\"55.615234\" xlink:href=\"#LiberationSans-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#LiberationSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 24.6 182.82716 \nL 405.054545 182.82716 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\"/>\n     <g id=\"text_8\">\n      <!-- 0.4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 186.450598)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-48\"/>\n       <use x=\"55.615234\" xlink:href=\"#LiberationSans-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#LiberationSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 24.6 132.737389 \nL 405.054545 132.737389 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\"/>\n     <g id=\"text_9\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 51.21875 22.515625 \nQ 51.21875 11.625 45.3125 5.3125 \nQ 39.40625 -0.984375 29 -0.984375 \nQ 17.390625 -0.984375 11.234375 7.65625 \nQ 5.078125 16.3125 5.078125 32.8125 \nQ 5.078125 50.6875 11.46875 60.25 \nQ 17.875 69.828125 29.6875 69.828125 \nQ 45.265625 69.828125 49.3125 55.8125 \nL 40.921875 54.296875 \nQ 38.328125 62.703125 29.59375 62.703125 \nQ 22.078125 62.703125 17.9375 55.6875 \nQ 13.8125 48.6875 13.8125 35.40625 \nQ 16.21875 39.84375 20.5625 42.15625 \nQ 24.90625 44.484375 30.515625 44.484375 \nQ 40.046875 44.484375 45.625 38.515625 \nQ 51.21875 32.5625 51.21875 22.515625 \nz\nM 42.28125 22.125 \nQ 42.28125 29.59375 38.625 33.640625 \nQ 34.96875 37.703125 28.421875 37.703125 \nQ 22.265625 37.703125 18.484375 34.109375 \nQ 14.703125 30.515625 14.703125 24.21875 \nQ 14.703125 16.265625 18.625 11.1875 \nQ 22.5625 6.109375 28.71875 6.109375 \nQ 35.0625 6.109375 38.671875 10.375 \nQ 42.28125 14.65625 42.28125 22.125 \nz\n\" id=\"LiberationSans-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 136.360826)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-48\"/>\n       <use x=\"55.615234\" xlink:href=\"#LiberationSans-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#LiberationSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 24.6 82.647618 \nL 405.054545 82.647618 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\"/>\n     <g id=\"text_10\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 51.265625 19.1875 \nQ 51.265625 9.671875 45.203125 4.34375 \nQ 39.15625 -0.984375 27.828125 -0.984375 \nQ 16.796875 -0.984375 10.5625 4.234375 \nQ 4.34375 9.46875 4.34375 19.09375 \nQ 4.34375 25.828125 8.203125 30.421875 \nQ 12.0625 35.015625 18.0625 35.984375 \nL 18.0625 36.1875 \nQ 12.453125 37.5 9.203125 41.890625 \nQ 5.953125 46.296875 5.953125 52.203125 \nQ 5.953125 60.0625 11.828125 64.9375 \nQ 17.71875 69.828125 27.640625 69.828125 \nQ 37.796875 69.828125 43.671875 65.03125 \nQ 49.5625 60.25 49.5625 52.09375 \nQ 49.5625 46.1875 46.28125 41.796875 \nQ 43.015625 37.40625 37.359375 36.28125 \nL 37.359375 36.078125 \nQ 43.953125 35.015625 47.609375 30.5 \nQ 51.265625 25.984375 51.265625 19.1875 \nz\nM 40.4375 51.609375 \nQ 40.4375 63.28125 27.640625 63.28125 \nQ 21.4375 63.28125 18.1875 60.34375 \nQ 14.9375 57.421875 14.9375 51.609375 \nQ 14.9375 45.703125 18.28125 42.59375 \nQ 21.625 39.5 27.734375 39.5 \nQ 33.9375 39.5 37.1875 42.359375 \nQ 40.4375 45.21875 40.4375 51.609375 \nz\nM 42.140625 20.015625 \nQ 42.140625 26.421875 38.328125 29.65625 \nQ 34.515625 32.90625 27.640625 32.90625 \nQ 20.953125 32.90625 17.1875 29.40625 \nQ 13.421875 25.921875 13.421875 19.828125 \nQ 13.421875 5.609375 27.9375 5.609375 \nQ 35.109375 5.609375 38.625 9.046875 \nQ 42.140625 12.5 42.140625 20.015625 \nz\n\" id=\"LiberationSans-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 86.271055)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-48\"/>\n       <use x=\"55.615234\" xlink:href=\"#LiberationSans-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#LiberationSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 24.6 32.557846 \nL 405.054545 32.557846 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\"/>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 7.625 0 \nL 7.625 7.46875 \nL 25.140625 7.46875 \nL 25.140625 60.40625 \nL 9.625 49.3125 \nL 9.625 57.625 \nL 25.875 68.796875 \nL 33.984375 68.796875 \nL 33.984375 7.46875 \nL 50.734375 7.46875 \nL 50.734375 0 \nz\n\" id=\"LiberationSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 36.181284)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-49\"/>\n       <use x=\"55.615234\" xlink:href=\"#LiberationSans-46\"/>\n       <use x=\"83.398438\" xlink:href=\"#LiberationSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#p0b2bc3a5cc)\" d=\"M 41.893388 281.330486 \nL 54.010878 280.634691 \nL 64.397297 279.815396 \nL 73.398861 278.883402 \nL 81.361782 277.839117 \nL 88.632276 276.662156 \nL 95.210342 275.375017 \nL 101.442193 273.924496 \nL 106.981617 272.415278 \nL 112.174827 270.784868 \nL 117.368037 268.91822 \nL 122.215032 266.936933 \nL 126.715814 264.867572 \nL 131.216596 262.554985 \nL 135.717378 259.976791 \nL 139.871945 257.341447 \nL 144.026513 254.442414 \nL 148.181081 251.262165 \nL 152.335649 247.784045 \nL 156.490217 243.992822 \nL 160.644784 239.875304 \nL 164.799352 235.421028 \nL 168.95392 230.622979 \nL 173.108488 225.478337 \nL 177.263056 219.989193 \nL 181.763837 213.662875 \nL 186.264619 206.959142 \nL 191.111615 199.348501 \nL 196.651039 190.211172 \nL 202.88289 179.475548 \nL 211.192026 164.662682 \nL 228.502725 133.668383 \nL 234.734577 123.029196 \nL 240.274 114.002494 \nL 245.120996 106.503949 \nL 249.967992 99.422645 \nL 254.468774 93.246704 \nL 258.623342 87.900505 \nL 262.777909 82.90032 \nL 266.932477 78.24611 \nL 271.087045 73.933217 \nL 275.241613 69.9531 \nL 279.39618 66.294072 \nL 283.550748 62.94201 \nL 287.705316 59.88102 \nL 291.859884 57.094021 \nL 296.014452 54.563263 \nL 300.515233 52.089911 \nL 305.016015 49.873485 \nL 309.863011 47.748547 \nL 314.710007 45.869488 \nL 319.903216 44.10074 \nL 325.44264 42.461541 \nL 331.328278 40.96522 \nL 337.56013 39.619324 \nL 344.138195 38.42603 \nL 351.408689 37.335708 \nL 359.37161 36.368957 \nL 368.373174 35.506683 \nL 378.413379 34.770913 \nL 387.761157 34.250795 \nL 387.761157 34.250795 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m7d43c597a3\" style=\"stroke:#ff0000;\"/>\n    </defs>\n    <g clip-path=\"url(#p0b2bc3a5cc)\">\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"184.426734\" xlink:href=\"#m7d43c597a3\" y=\"209.740683\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 24.6 293.69625 \nL 24.6 21.89625 \n\" style=\"fill:none;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 405.054545 293.69625 \nL 405.054545 21.89625 \n\" style=\"fill:none;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 24.6 293.69625 \nL 405.054545 293.69625 \n\" style=\"fill:none;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 24.6 21.89625 \nL 405.054545 21.89625 \n\" style=\"fill:none;\"/>\n   </g>\n   <g id=\"text_12\">\n    <!-- Sigmoid of Weighted Sum -->\n    <defs>\n     <path d=\"M 62.109375 19 \nQ 62.109375 9.46875 54.65625 4.234375 \nQ 47.21875 -0.984375 33.6875 -0.984375 \nQ 8.546875 -0.984375 4.546875 16.5 \nL 13.578125 18.3125 \nQ 15.140625 12.109375 20.21875 9.203125 \nQ 25.296875 6.296875 34.03125 6.296875 \nQ 43.0625 6.296875 47.96875 9.390625 \nQ 52.875 12.5 52.875 18.5 \nQ 52.875 21.875 51.34375 23.96875 \nQ 49.8125 26.078125 47.015625 27.4375 \nQ 44.234375 28.8125 40.375 29.734375 \nQ 36.53125 30.671875 31.84375 31.734375 \nQ 23.6875 33.546875 19.453125 35.34375 \nQ 15.234375 37.15625 12.796875 39.375 \nQ 10.359375 41.609375 9.0625 44.578125 \nQ 7.765625 47.5625 7.765625 51.421875 \nQ 7.765625 60.25 14.53125 65.03125 \nQ 21.296875 69.828125 33.890625 69.828125 \nQ 45.609375 69.828125 51.8125 66.234375 \nQ 58.015625 62.640625 60.5 54 \nL 51.3125 52.390625 \nQ 49.8125 57.859375 45.5625 60.328125 \nQ 41.3125 62.796875 33.796875 62.796875 \nQ 25.53125 62.796875 21.1875 60.0625 \nQ 16.84375 57.328125 16.84375 51.90625 \nQ 16.84375 48.734375 18.53125 46.65625 \nQ 20.21875 44.578125 23.390625 43.140625 \nQ 26.5625 41.703125 36.03125 39.59375 \nQ 39.203125 38.875 42.359375 38.109375 \nQ 45.515625 37.359375 48.390625 36.296875 \nQ 51.265625 35.25 53.78125 33.828125 \nQ 56.296875 32.421875 58.15625 30.375 \nQ 60.015625 28.328125 61.0625 25.53125 \nQ 62.109375 22.75 62.109375 19 \nz\n\" id=\"LiberationSans-83\"/>\n     <path d=\"M 6.6875 64.0625 \nL 6.6875 72.46875 \nL 15.484375 72.46875 \nL 15.484375 64.0625 \nz\nM 6.6875 0 \nL 6.6875 52.828125 \nL 15.484375 52.828125 \nL 15.484375 0 \nz\n\" id=\"LiberationSans-105\"/>\n     <path d=\"M 26.765625 -20.75 \nQ 18.109375 -20.75 12.984375 -17.359375 \nQ 7.859375 -13.96875 6.390625 -7.71875 \nL 15.234375 -6.453125 \nQ 16.109375 -10.109375 19.109375 -12.078125 \nQ 22.125 -14.0625 27 -14.0625 \nQ 40.140625 -14.0625 40.140625 1.3125 \nL 40.140625 9.8125 \nL 40.046875 9.8125 \nQ 37.546875 4.734375 33.203125 2.171875 \nQ 28.859375 -0.390625 23.046875 -0.390625 \nQ 13.328125 -0.390625 8.765625 6.046875 \nQ 4.203125 12.5 4.203125 26.3125 \nQ 4.203125 40.328125 9.109375 46.984375 \nQ 14.015625 53.65625 24.03125 53.65625 \nQ 29.640625 53.65625 33.765625 51.09375 \nQ 37.890625 48.53125 40.140625 43.796875 \nL 40.234375 43.796875 \nQ 40.234375 45.265625 40.421875 48.875 \nQ 40.625 52.484375 40.828125 52.828125 \nL 49.171875 52.828125 \nQ 48.875 50.203125 48.875 41.890625 \nL 48.875 1.515625 \nQ 48.875 -20.75 26.765625 -20.75 \nz\nM 40.140625 26.421875 \nQ 40.140625 32.859375 38.375 37.515625 \nQ 36.625 42.1875 33.421875 44.65625 \nQ 30.21875 47.125 26.171875 47.125 \nQ 19.4375 47.125 16.359375 42.234375 \nQ 13.28125 37.359375 13.28125 26.421875 \nQ 13.28125 15.578125 16.15625 10.84375 \nQ 19.046875 6.109375 26.03125 6.109375 \nQ 30.171875 6.109375 33.390625 8.546875 \nQ 36.625 10.984375 38.375 15.546875 \nQ 40.140625 20.125 40.140625 26.421875 \nz\n\" id=\"LiberationSans-103\"/>\n     <path d=\"M 37.5 0 \nL 37.5 33.5 \nQ 37.5 41.15625 35.390625 44.078125 \nQ 33.296875 47.015625 27.828125 47.015625 \nQ 22.21875 47.015625 18.9375 42.71875 \nQ 15.671875 38.421875 15.671875 30.609375 \nL 15.671875 0 \nL 6.9375 0 \nL 6.9375 41.546875 \nQ 6.9375 50.78125 6.640625 52.828125 \nL 14.9375 52.828125 \nQ 14.984375 52.59375 15.03125 51.515625 \nQ 15.09375 50.4375 15.15625 49.046875 \nQ 15.234375 47.65625 15.328125 43.796875 \nL 15.484375 43.796875 \nQ 18.3125 49.421875 21.96875 51.609375 \nQ 25.640625 53.8125 30.90625 53.8125 \nQ 36.921875 53.8125 40.40625 51.421875 \nQ 43.890625 49.03125 45.265625 43.796875 \nL 45.40625 43.796875 \nQ 48.140625 49.125 52.015625 51.46875 \nQ 55.90625 53.8125 61.421875 53.8125 \nQ 69.4375 53.8125 73.0625 49.46875 \nQ 76.703125 45.125 76.703125 35.203125 \nL 76.703125 0 \nL 68.015625 0 \nL 68.015625 33.5 \nQ 68.015625 41.15625 65.90625 44.078125 \nQ 63.8125 47.015625 58.34375 47.015625 \nQ 52.59375 47.015625 49.390625 42.75 \nQ 46.1875 38.484375 46.1875 30.609375 \nL 46.1875 0 \nz\n\" id=\"LiberationSans-109\"/>\n     <path d=\"M 51.421875 26.46875 \nQ 51.421875 12.59375 45.3125 5.796875 \nQ 39.203125 -0.984375 27.59375 -0.984375 \nQ 16.015625 -0.984375 10.109375 6.078125 \nQ 4.203125 13.140625 4.203125 26.46875 \nQ 4.203125 53.8125 27.875 53.8125 \nQ 39.984375 53.8125 45.703125 47.140625 \nQ 51.421875 40.484375 51.421875 26.46875 \nz\nM 42.1875 26.46875 \nQ 42.1875 37.40625 38.9375 42.359375 \nQ 35.6875 47.3125 28.03125 47.3125 \nQ 20.3125 47.3125 16.859375 42.25 \nQ 13.421875 37.203125 13.421875 26.46875 \nQ 13.421875 16.015625 16.8125 10.765625 \nQ 20.21875 5.515625 27.484375 5.515625 \nQ 35.40625 5.515625 38.796875 10.59375 \nQ 42.1875 15.671875 42.1875 26.46875 \nz\n\" id=\"LiberationSans-111\"/>\n     <path d=\"M 40.09375 8.5 \nQ 37.640625 3.421875 33.609375 1.21875 \nQ 29.59375 -0.984375 23.640625 -0.984375 \nQ 13.625 -0.984375 8.90625 5.75 \nQ 4.203125 12.5 4.203125 26.171875 \nQ 4.203125 53.8125 23.640625 53.8125 \nQ 29.640625 53.8125 33.640625 51.609375 \nQ 37.640625 49.421875 40.09375 44.625 \nL 40.1875 44.625 \nL 40.09375 50.53125 \nL 40.09375 72.46875 \nL 48.875 72.46875 \nL 48.875 10.890625 \nQ 48.875 2.640625 49.171875 0 \nL 40.765625 0 \nQ 40.625 0.78125 40.453125 3.609375 \nQ 40.28125 6.453125 40.28125 8.5 \nz\nM 13.421875 26.46875 \nQ 13.421875 15.375 16.34375 10.59375 \nQ 19.28125 5.8125 25.875 5.8125 \nQ 33.34375 5.8125 36.71875 10.984375 \nQ 40.09375 16.15625 40.09375 27.046875 \nQ 40.09375 37.546875 36.71875 42.421875 \nQ 33.34375 47.3125 25.984375 47.3125 \nQ 19.34375 47.3125 16.375 42.40625 \nQ 13.421875 37.5 13.421875 26.46875 \nz\n\" id=\"LiberationSans-100\"/>\n     <path id=\"LiberationSans-32\"/>\n     <path d=\"M 17.625 46.4375 \nL 17.625 0 \nL 8.84375 0 \nL 8.84375 46.4375 \nL 1.421875 46.4375 \nL 1.421875 52.828125 \nL 8.84375 52.828125 \nL 8.84375 58.796875 \nQ 8.84375 66.015625 12.015625 69.1875 \nQ 15.1875 72.359375 21.734375 72.359375 \nQ 25.390625 72.359375 27.9375 71.78125 \nL 27.9375 65.09375 \nQ 25.734375 65.484375 24.03125 65.484375 \nQ 20.65625 65.484375 19.140625 63.765625 \nQ 17.625 62.0625 17.625 57.5625 \nL 17.625 52.828125 \nL 27.9375 52.828125 \nL 27.9375 46.4375 \nz\n\" id=\"LiberationSans-102\"/>\n     <path d=\"M 73.78125 0 \nL 62.640625 0 \nL 50.734375 43.703125 \nQ 49.5625 47.796875 47.3125 58.40625 \nQ 46.046875 52.734375 45.15625 48.921875 \nQ 44.28125 45.125 31.84375 0 \nL 20.703125 0 \nL 0.4375 68.796875 \nL 10.15625 68.796875 \nL 22.515625 25.09375 \nQ 24.703125 16.890625 26.5625 8.203125 \nQ 27.734375 13.578125 29.265625 19.921875 \nQ 30.8125 26.265625 42.828125 68.796875 \nL 51.765625 68.796875 \nL 63.71875 25.984375 \nQ 66.453125 15.484375 68.015625 8.203125 \nL 68.453125 9.90625 \nQ 69.78125 15.53125 70.609375 19.0625 \nQ 71.4375 22.609375 84.328125 68.796875 \nL 94.046875 68.796875 \nz\n\" id=\"LiberationSans-87\"/>\n     <path d=\"M 13.484375 24.5625 \nQ 13.484375 15.484375 17.234375 10.546875 \nQ 21 5.609375 28.21875 5.609375 \nQ 33.9375 5.609375 37.375 7.90625 \nQ 40.828125 10.203125 42.046875 13.71875 \nL 49.75 11.53125 \nQ 45.015625 -0.984375 28.21875 -0.984375 \nQ 16.5 -0.984375 10.375 6 \nQ 4.25 12.984375 4.25 26.765625 \nQ 4.25 39.84375 10.375 46.828125 \nQ 16.5 53.8125 27.875 53.8125 \nQ 51.171875 53.8125 51.171875 25.734375 \nL 51.171875 24.5625 \nz\nM 42.09375 31.296875 \nQ 41.359375 39.65625 37.84375 43.484375 \nQ 34.328125 47.3125 27.734375 47.3125 \nQ 21.34375 47.3125 17.609375 43.03125 \nQ 13.875 38.765625 13.578125 31.296875 \nz\n\" id=\"LiberationSans-101\"/>\n     <path d=\"M 15.484375 43.796875 \nQ 18.3125 48.96875 22.28125 51.390625 \nQ 26.265625 53.8125 32.375 53.8125 \nQ 40.96875 53.8125 45.046875 49.53125 \nQ 49.125 45.265625 49.125 35.203125 \nL 49.125 0 \nL 40.28125 0 \nL 40.28125 33.5 \nQ 40.28125 39.0625 39.25 41.765625 \nQ 38.234375 44.484375 35.890625 45.75 \nQ 33.546875 47.015625 29.390625 47.015625 \nQ 23.1875 47.015625 19.453125 42.71875 \nQ 15.71875 38.421875 15.71875 31.15625 \nL 15.71875 0 \nL 6.9375 0 \nL 6.9375 72.46875 \nL 15.71875 72.46875 \nL 15.71875 53.609375 \nQ 15.71875 50.640625 15.546875 47.453125 \nQ 15.375 44.28125 15.328125 43.796875 \nz\n\" id=\"LiberationSans-104\"/>\n     <path d=\"M 27.046875 0.390625 \nQ 22.703125 -0.78125 18.171875 -0.78125 \nQ 7.625 -0.78125 7.625 11.1875 \nL 7.625 46.4375 \nL 1.515625 46.4375 \nL 1.515625 52.828125 \nL 7.953125 52.828125 \nL 10.546875 64.65625 \nL 16.40625 64.65625 \nL 16.40625 52.828125 \nL 26.171875 52.828125 \nL 26.171875 46.4375 \nL 16.40625 46.4375 \nL 16.40625 13.09375 \nQ 16.40625 9.28125 17.640625 7.734375 \nQ 18.890625 6.203125 21.96875 6.203125 \nQ 23.734375 6.203125 27.046875 6.890625 \nz\n\" id=\"LiberationSans-116\"/>\n     <path d=\"M 15.328125 52.828125 \nL 15.328125 19.34375 \nQ 15.328125 14.109375 16.359375 11.21875 \nQ 17.390625 8.34375 19.625 7.078125 \nQ 21.875 5.8125 26.21875 5.8125 \nQ 32.5625 5.8125 36.21875 10.15625 \nQ 39.890625 14.5 39.890625 22.21875 \nL 39.890625 52.828125 \nL 48.6875 52.828125 \nL 48.6875 11.28125 \nQ 48.6875 2.046875 48.96875 0 \nL 40.671875 0 \nQ 40.625 0.25 40.578125 1.3125 \nQ 40.53125 2.390625 40.453125 3.78125 \nQ 40.375 5.171875 40.28125 9.03125 \nL 40.140625 9.03125 \nQ 37.109375 3.5625 33.125 1.28125 \nQ 29.15625 -0.984375 23.25 -0.984375 \nQ 14.546875 -0.984375 10.515625 3.34375 \nQ 6.5 7.671875 6.5 17.625 \nL 6.5 52.828125 \nz\n\" id=\"LiberationSans-117\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(145.56946 15.89625)scale(0.12 -0.12)\">\n     <use xlink:href=\"#LiberationSans-83\"/>\n     <use x=\"66.699219\" xlink:href=\"#LiberationSans-105\"/>\n     <use x=\"88.916016\" xlink:href=\"#LiberationSans-103\"/>\n     <use x=\"144.53125\" xlink:href=\"#LiberationSans-109\"/>\n     <use x=\"227.832031\" xlink:href=\"#LiberationSans-111\"/>\n     <use x=\"283.447266\" xlink:href=\"#LiberationSans-105\"/>\n     <use x=\"305.664062\" xlink:href=\"#LiberationSans-100\"/>\n     <use x=\"361.279297\" xlink:href=\"#LiberationSans-32\"/>\n     <use x=\"389.0625\" xlink:href=\"#LiberationSans-111\"/>\n     <use x=\"444.677734\" xlink:href=\"#LiberationSans-102\"/>\n     <use x=\"472.460938\" xlink:href=\"#LiberationSans-32\"/>\n     <use x=\"500.244141\" xlink:href=\"#LiberationSans-87\"/>\n     <use x=\"592.878906\" xlink:href=\"#LiberationSans-101\"/>\n     <use x=\"648.494141\" xlink:href=\"#LiberationSans-105\"/>\n     <use x=\"670.710938\" xlink:href=\"#LiberationSans-103\"/>\n     <use x=\"726.326172\" xlink:href=\"#LiberationSans-104\"/>\n     <use x=\"781.941406\" xlink:href=\"#LiberationSans-116\"/>\n     <use x=\"809.724609\" xlink:href=\"#LiberationSans-101\"/>\n     <use x=\"865.339844\" xlink:href=\"#LiberationSans-100\"/>\n     <use x=\"920.955078\" xlink:href=\"#LiberationSans-32\"/>\n     <use x=\"948.738281\" xlink:href=\"#LiberationSans-83\"/>\n     <use x=\"1015.4375\" xlink:href=\"#LiberationSans-117\"/>\n     <use x=\"1071.052734\" xlink:href=\"#LiberationSans-109\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 481.145455 293.69625 \nL 861.6 293.69625 \nL 861.6 21.89625 \nL 481.145455 21.89625 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_6\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 533.060241 293.69625 \nL 533.060241 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\"/>\n     <g id=\"text_13\">\n      <!-- −4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(527.35946 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-8722\"/>\n       <use x=\"58.398438\" xlink:href=\"#LiberationSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 602.303038 293.69625 \nL 602.303038 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\"/>\n     <g id=\"text_14\">\n      <!-- −2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(596.602256 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-8722\"/>\n       <use x=\"58.398438\" xlink:href=\"#LiberationSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 671.545834 293.69625 \nL 671.545834 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\"/>\n     <g id=\"text_15\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(668.765366 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 740.788631 293.69625 \nL 740.788631 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\"/>\n     <g id=\"text_16\">\n      <!-- 2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(738.008162 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 810.031427 293.69625 \nL 810.031427 21.89625 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\"/>\n     <g id=\"text_17\">\n      <!-- 4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(807.250959 304.443125)scale(0.1 -0.1)\">\n       <use xlink:href=\"#LiberationSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_35\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 481.145455 283.006703 \nL 861.6 283.006703 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_36\"/>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_37\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 481.145455 232.916931 \nL 861.6 232.916931 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_38\"/>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_39\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 481.145455 182.82716 \nL 861.6 182.82716 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_40\"/>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_41\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 481.145455 132.737389 \nL 861.6 132.737389 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_42\"/>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_43\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 481.145455 82.647618 \nL 861.6 82.647618 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_44\"/>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_45\">\n      <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 481.145455 32.557846 \nL 861.6 32.557846 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_46\"/>\n    </g>\n   </g>\n   <g id=\"line2d_47\">\n    <path clip-path=\"url(#p9d4d2e1309)\" d=\"M 498.438843 281.341705 \nL 510.556332 280.657156 \nL 520.942752 279.856061 \nL 530.290529 278.911884 \nL 538.599665 277.847805 \nL 546.216373 276.644376 \nL 553.140652 275.324724 \nL 559.718718 273.839204 \nL 565.95057 272.194111 \nL 571.836207 270.40259 \nL 577.375631 268.484727 \nL 582.915055 266.324706 \nL 588.108264 264.067419 \nL 593.301474 261.577813 \nL 598.840898 258.6644 \nL 604.380322 255.492273 \nL 610.265959 251.860943 \nL 616.844025 247.537037 \nL 626.191803 241.097546 \nL 636.578222 233.993458 \nL 642.117646 230.469339 \nL 646.618428 227.856157 \nL 650.772995 225.707951 \nL 654.235135 224.150292 \nL 657.697275 222.833674 \nL 660.813201 221.874978 \nL 663.929127 221.146027 \nL 667.045052 220.658282 \nL 670.160978 220.419527 \nL 672.93069 220.419527 \nL 675.700402 220.619353 \nL 678.816328 221.079746 \nL 681.932254 221.782398 \nL 685.04818 222.716239 \nL 688.510319 224.007352 \nL 691.972459 225.542105 \nL 695.780813 227.478967 \nL 699.935381 229.843362 \nL 704.782377 232.862669 \nL 711.014228 237.023991 \nL 733.171923 252.081343 \nL 739.057561 255.697805 \nL 744.596985 258.854202 \nL 750.136408 261.751084 \nL 755.329618 264.225078 \nL 760.522828 266.467084 \nL 766.062252 268.611524 \nL 771.601675 270.514794 \nL 777.487313 272.292059 \nL 783.372951 273.839204 \nL 789.604802 275.252457 \nL 796.182868 276.522684 \nL 803.453362 277.697499 \nL 811.416283 278.750549 \nL 820.071633 279.666502 \nL 829.765624 280.46548 \nL 840.844472 281.150991 \nL 844.306612 281.325197 \nL 844.306612 281.325197 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <g clip-path=\"url(#p9d4d2e1309)\">\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"640.972188\" xlink:href=\"#m7d43c597a3\" y=\"231.17384\"/>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 481.145455 293.69625 \nL 481.145455 21.89625 \n\" style=\"fill:none;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 861.6 293.69625 \nL 861.6 21.89625 \n\" style=\"fill:none;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 481.145455 293.69625 \nL 861.6 293.69625 \n\" style=\"fill:none;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 481.145455 21.89625 \nL 861.6 21.89625 \n\" style=\"fill:none;\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- Sigmoid Derivative of Weighted Sum -->\n    <defs>\n     <path d=\"M 67.4375 35.109375 \nQ 67.4375 24.46875 63.28125 16.484375 \nQ 59.125 8.5 51.5 4.25 \nQ 43.890625 0 33.9375 0 \nL 8.203125 0 \nL 8.203125 68.796875 \nL 30.953125 68.796875 \nQ 48.4375 68.796875 57.9375 60.03125 \nQ 67.4375 51.265625 67.4375 35.109375 \nz\nM 58.0625 35.109375 \nQ 58.0625 47.90625 51.046875 54.609375 \nQ 44.046875 61.328125 30.765625 61.328125 \nL 17.53125 61.328125 \nL 17.53125 7.46875 \nL 32.859375 7.46875 \nQ 40.4375 7.46875 46.171875 10.78125 \nQ 51.90625 14.109375 54.984375 20.359375 \nQ 58.0625 26.609375 58.0625 35.109375 \nz\n\" id=\"LiberationSans-68\"/>\n     <path d=\"M 6.9375 0 \nL 6.9375 40.53125 \nQ 6.9375 46.09375 6.640625 52.828125 \nL 14.9375 52.828125 \nQ 15.328125 43.84375 15.328125 42.046875 \nL 15.53125 42.046875 \nQ 17.625 48.828125 20.359375 51.3125 \nQ 23.09375 53.8125 28.078125 53.8125 \nQ 29.828125 53.8125 31.640625 53.328125 \nL 31.640625 45.265625 \nQ 29.890625 45.75 26.953125 45.75 \nQ 21.484375 45.75 18.59375 41.03125 \nQ 15.71875 36.328125 15.71875 27.546875 \nL 15.71875 0 \nz\n\" id=\"LiberationSans-114\"/>\n     <path d=\"M 29.9375 0 \nL 19.53125 0 \nL 0.34375 52.828125 \nL 9.71875 52.828125 \nL 21.34375 18.453125 \nQ 21.96875 16.5 24.703125 6.890625 \nL 26.421875 12.59375 \nL 28.328125 18.359375 \nL 40.328125 52.828125 \nL 49.65625 52.828125 \nz\n\" id=\"LiberationSans-118\"/>\n     <path d=\"M 20.21875 -0.984375 \nQ 12.25 -0.984375 8.25 3.21875 \nQ 4.25 7.421875 4.25 14.75 \nQ 4.25 22.953125 9.640625 27.34375 \nQ 15.046875 31.734375 27.046875 32.03125 \nL 38.921875 32.234375 \nL 38.921875 35.109375 \nQ 38.921875 41.546875 36.1875 44.328125 \nQ 33.453125 47.125 27.59375 47.125 \nQ 21.6875 47.125 19 45.109375 \nQ 16.3125 43.109375 15.765625 38.71875 \nL 6.59375 39.546875 \nQ 8.84375 53.8125 27.78125 53.8125 \nQ 37.75 53.8125 42.765625 49.234375 \nQ 47.796875 44.671875 47.796875 36.03125 \nL 47.796875 13.28125 \nQ 47.796875 9.375 48.828125 7.390625 \nQ 49.859375 5.421875 52.734375 5.421875 \nQ 54 5.421875 55.609375 5.765625 \nL 55.609375 0.296875 \nQ 52.296875 -0.484375 48.828125 -0.484375 \nQ 43.953125 -0.484375 41.71875 2.078125 \nQ 39.5 4.640625 39.203125 10.109375 \nL 38.921875 10.109375 \nQ 35.546875 4.046875 31.078125 1.53125 \nQ 26.609375 -0.984375 20.21875 -0.984375 \nz\nM 22.21875 5.609375 \nQ 27.046875 5.609375 30.8125 7.8125 \nQ 34.578125 10.015625 36.75 13.84375 \nQ 38.921875 17.671875 38.921875 21.734375 \nL 38.921875 26.078125 \nL 29.296875 25.875 \nQ 23.09375 25.78125 19.890625 24.609375 \nQ 16.703125 23.4375 14.984375 21 \nQ 13.28125 18.5625 13.28125 14.59375 \nQ 13.28125 10.296875 15.59375 7.953125 \nQ 17.921875 5.609375 22.21875 5.609375 \nz\n\" id=\"LiberationSans-97\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(573.77429 15.89625)scale(0.12 -0.12)\">\n     <use xlink:href=\"#LiberationSans-83\"/>\n     <use x=\"66.699219\" xlink:href=\"#LiberationSans-105\"/>\n     <use x=\"88.916016\" xlink:href=\"#LiberationSans-103\"/>\n     <use x=\"144.53125\" xlink:href=\"#LiberationSans-109\"/>\n     <use x=\"227.832031\" xlink:href=\"#LiberationSans-111\"/>\n     <use x=\"283.447266\" xlink:href=\"#LiberationSans-105\"/>\n     <use x=\"305.664062\" xlink:href=\"#LiberationSans-100\"/>\n     <use x=\"361.279297\" xlink:href=\"#LiberationSans-32\"/>\n     <use x=\"389.0625\" xlink:href=\"#LiberationSans-68\"/>\n     <use x=\"461.279297\" xlink:href=\"#LiberationSans-101\"/>\n     <use x=\"516.894531\" xlink:href=\"#LiberationSans-114\"/>\n     <use x=\"550.195312\" xlink:href=\"#LiberationSans-105\"/>\n     <use x=\"572.412109\" xlink:href=\"#LiberationSans-118\"/>\n     <use x=\"622.412109\" xlink:href=\"#LiberationSans-97\"/>\n     <use x=\"678.027344\" xlink:href=\"#LiberationSans-116\"/>\n     <use x=\"705.810547\" xlink:href=\"#LiberationSans-105\"/>\n     <use x=\"728.027344\" xlink:href=\"#LiberationSans-118\"/>\n     <use x=\"778.027344\" xlink:href=\"#LiberationSans-101\"/>\n     <use x=\"833.642578\" xlink:href=\"#LiberationSans-32\"/>\n     <use x=\"861.425781\" xlink:href=\"#LiberationSans-111\"/>\n     <use x=\"917.041016\" xlink:href=\"#LiberationSans-102\"/>\n     <use x=\"944.824219\" xlink:href=\"#LiberationSans-32\"/>\n     <use x=\"972.607422\" xlink:href=\"#LiberationSans-87\"/>\n     <use x=\"1065.242188\" xlink:href=\"#LiberationSans-101\"/>\n     <use x=\"1120.857422\" xlink:href=\"#LiberationSans-105\"/>\n     <use x=\"1143.074219\" xlink:href=\"#LiberationSans-103\"/>\n     <use x=\"1198.689453\" xlink:href=\"#LiberationSans-104\"/>\n     <use x=\"1254.304688\" xlink:href=\"#LiberationSans-116\"/>\n     <use x=\"1282.087891\" xlink:href=\"#LiberationSans-101\"/>\n     <use x=\"1337.703125\" xlink:href=\"#LiberationSans-100\"/>\n     <use x=\"1393.318359\" xlink:href=\"#LiberationSans-32\"/>\n     <use x=\"1421.101562\" xlink:href=\"#LiberationSans-83\"/>\n     <use x=\"1487.800781\" xlink:href=\"#LiberationSans-117\"/>\n     <use x=\"1543.416016\" xlink:href=\"#LiberationSans-109\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0b2bc3a5cc\">\n   <rect height=\"271.8\" width=\"380.454545\" x=\"24.6\" y=\"21.89625\"/>\n  </clipPath>\n  <clipPath id=\"p9d4d2e1309\">\n   <rect height=\"271.8\" width=\"380.454545\" x=\"481.145455\" y=\"21.89625\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# call regplot on each axes\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(15,5))\n",
    "sns.lineplot(x=line_x, y=y, ax=ax1)\n",
    "ax1.plot(x[0], s[0], 'ro')\n",
    "ax1.set_title(\"Sigmoid of Weighted Sum\")\n",
    "sns.lineplot(x=line_x, y=y_d, ax=ax2) \n",
    "ax2.plot(x[0],sx[0],'ro');\n",
    "ax2.set_title(\"Sigmoid Derivative of Weighted Sum\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the derivate graph. The derivative multiplied by the error tells us where to assign blame and update the weights most effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Error\n",
    "Justice hasn't been served yet - tho. We still have neurons to blame. Let's go back another layer. \n",
    "\n",
    "`self.z2_error = self.o_delta.dot(self.weights2.T)`\n",
    "\n",
    "__Discussion:__ Why is this shape different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.23952061, -0.19347426,  0.1111231 ],\n       [-0.1847631 , -0.14924354,  0.08571892],\n       [-0.21246478, -0.17161974,  0.09857082]])"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "nn.o_delta.dot(nn.weights2.T)\n",
    "# how wrong was the nn for x[0] for hidden node h2? the error for each nuron in that hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Gradient\n",
    "For each observation, how much more sigmoid activation from this layer would have pushed us towards the right answer?\n",
    "\n",
    "`self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.05483346, -0.04408088,  0.01744341],\n       [-0.03871267, -0.0311213 ,  0.01231513],\n       [-0.04559029, -0.03665025,  0.01450301]])"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "nn.z2_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "X.T.shape == nn.weights1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.66666667, 1.        ],\n       [0.33333333, 0.55555556],\n       [1.        , 0.66666667]])"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descent\n",
    "\n",
    "*Discussion:* Input to Hidden Weight Update\n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.66666667, 0.33333333, 1.        ],\n       [1.        , 0.55555556, 0.66666667]])"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.03943374, -0.05508949,  0.00386508],\n       [ 0.04445043, -0.06209788,  0.00435679]])"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "X.T.dot(nn.z2_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion:* Hidden to Output Weight Update\n",
    "- Why is output the shape 3x1? \n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.12587682],\n       [0.21339318],\n       [0.18235102]])"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "nn.activated_hidden.T.dot(nn.o_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network (fo real this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------EPOCH 1---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.25814933]\n [0.33067192]\n [0.22642076]]\nLoss: \n 0.2975139279136551\n+---------EPOCH 2---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.3641282 ]\n [0.43154456]\n [0.3214717 ]]\nLoss: \n 0.19978482310830625\n+---------EPOCH 3---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.45923322]\n [0.51595304]\n [0.40906549]]\nLoss: \n 0.12958010176535534\n+---------EPOCH 4---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.53596514]\n [0.58131764]\n [0.48180761]]\nLoss: \n 0.08433352590924598\n+---------EPOCH 5---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.59522825]\n [0.63070626]\n [0.5395584 ]]\nLoss: \n 0.05622967401952095\n+---------EPOCH 500---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.82873934]\n [0.80050493]\n [0.80075531]]\nLoss: \n 0.003853232317193865\n+---------EPOCH 1000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.82873657]\n [0.79594272]\n [0.80447936]]\nLoss: \n 0.0036219459645128013\n+---------EPOCH 1500---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.82925461]\n [0.79589156]\n [0.80328259]]\nLoss: \n 0.003591738608696659\n+---------EPOCH 2000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.83219558]\n [0.7947219 ]\n [0.80042151]]\nLoss: \n 0.003393659883290401\n+---------EPOCH 2500---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.83918755]\n [0.7865899 ]\n [0.80000598]]\nLoss: \n 0.0027107893500704556\n+---------EPOCH 3000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.8482196 ]\n [0.77235509]\n [0.80337698]]\nLoss: \n 0.0018112231518961125\n+---------EPOCH 3500---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.85669459]\n [0.76034251]\n [0.80541631]]\nLoss: \n 0.0011774044450064164\n+---------EPOCH 4000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.86418915]\n [0.75279332]\n [0.80451197]]\nLoss: \n 0.0007927255007351071\n+---------EPOCH 4500---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.87043335]\n [0.74778938]\n [0.80269071]]\nLoss: \n 0.0005512255230043662\n+---------EPOCH 5000---------+\nInput: \n [[0.66666667 1.        ]\n [0.33333333 0.55555556]\n [1.         0.66666667]]\nActual Output: \n [[0.9 ]\n [0.72]\n [0.8 ]]\nPredicted Output: \n [[0.87525816]\n [0.74390294]\n [0.80137031]]\nLoss: \n 0.00039512901019069755\n"
    }
   ],
   "source": [
    "# Train my 'net\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(5000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 500 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In the module project, you will implement backpropagation inside a multi-layer perceptron (aka a feedforward neural network). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Stochastic Gradient Descent (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
    "\n",
    "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF7UE-KluPsX"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "A true Stochastic GD-based implementation from [Welch Labs](https://www.youtube.com/watch?v=bxe2T-V8XRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when dealing with larger datasets it's not possible to calculate the gradient over the entire input field\n",
    "# besides speed it's possible to avoid local minima\n",
    "# when using minibatch SGD the adjustments of weights is higher\n",
    "# those are the modivations for using mini-batch SGD\n",
    "# not something that is done by hand in keras it's just a param that is passed.\n",
    "\n",
    "# slightly diffrent cost function\n",
    "# wrappers that help compute the gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-22177f8d7734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.evaluate(X,y)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate training object that makes the adjustments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA9LaTgKr6rP"
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_kHb6Se1u9y"
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYYVhFf4rn3q"
   },
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "L-gYdVfgrysE",
    "outputId": "ae371bf9-692c-49b4-b165-8562dab9c06e"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "shapes (4,4) and (1,3) not aligned: 4 (dim 1) != 1 (dim 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-5508d2e15b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-02d89162aed5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disp'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n\u001b[0;32m---> 30\u001b[0;31m                                  args=(X, y), options=options, callback=self.callbackF)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/intro_nn/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m/opt/conda/envs/intro_nn/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0mfunc_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m     \u001b[0mold_fval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfprime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/intro_nn/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/intro_nn/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-02d89162aed5>\u001b[0m in \u001b[0;36mcostFunctionWrapper\u001b[0;34m(self, params, X, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-537fef4a43c9>\u001b[0m in \u001b[0;36mcomputeGradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcomputeGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mdJdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdJdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunctionPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdJdW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdJdW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-537fef4a43c9>\u001b[0m in \u001b[0;36mcostFunctionPrime\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mdJdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mdelta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoidPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mdJdW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,4) and (1,3) not aligned: 4 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Jyv_L8Z2sKOA",
    "outputId": "08725651-6d21-401b-85c0-3487370b8bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[0.91991809]\n",
      " [0.85998334]\n",
      " [0.88999529]]\n",
      "Loss: \n",
      "2.3365414324478e-09\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "Gtf9WI9FtGPk",
    "outputId": "d062b2a3-5a92-403e-8ce0-c070aa79907b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "This is a reference implementation for you to explore. You will not be expected to apply it to today's module project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Sequential API (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that:\n",
    "\n",
    "> Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "Runs seamlessly on CPU and GPU.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIJoRBxHy27n"
   },
   "source": [
    "### Keras Perceptron Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "X = df[['x1', 'x2']].values\n",
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5216
    },
    "colab_type": "code",
    "id": "TQxyONqKvFxB",
    "outputId": "12966e66-2297-4f82-85b3-c275a9c38563"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/20\n1/1 [==============================] - 0s 1ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 2/20\n1/1 [==============================] - 0s 2ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 3/20\n1/1 [==============================] - 0s 975us/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 4/20\n1/1 [==============================] - 0s 1ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 5/20\n1/1 [==============================] - 0s 1ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 6/20\n1/1 [==============================] - 0s 2ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 7/20\n1/1 [==============================] - 0s 3ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 8/20\n1/1 [==============================] - 0s 1ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 9/20\n1/1 [==============================] - 0s 1000us/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 10/20\n1/1 [==============================] - 0s 2ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 11/20\n1/1 [==============================] - 0s 904us/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 12/20\n1/1 [==============================] - 0s 3ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 13/20\n1/1 [==============================] - 0s 1ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 14/20\n1/1 [==============================] - 0s 994us/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 15/20\n1/1 [==============================] - 0s 1ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 16/20\n1/1 [==============================] - 0s 1ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 17/20\n1/1 [==============================] - 0s 2ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 18/20\n1/1 [==============================] - 0s 1ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 19/20\n1/1 [==============================] - 0s 2ms/step - loss: 3.8123 - accuracy: 0.7500\nEpoch 20/20\n1/1 [==============================] - 0s 1ms/step - loss: 3.8123 - accuracy: 0.7500\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f84cc07c890>"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# This is our perceptron from Monday's by-hand: \n",
    "model = Sequential()\n",
    "model.add(Dense(1,input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(32,activation='softmax'))\n",
    "model.add(Dense(1,activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X,y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Z1wfKUxszPKa",
    "outputId": "0cdacd1d-6e5a-4bbe-fabb-568cd94724be"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1/1 [==============================] - 0s 922us/step - loss: 3.8123 - accuracy: 0.7500\naccuracy: 75.0\n"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "In the `Sequential` api model, you specify a model architecture by 'sequentially specifying layers. This type of specification works well for feed forward neural networks in which the data flows in one direction (forward propagation) and the error flows in the opposite direction (backwards propagation). The Keras `Sequential` API follows a standardarized worklow to estimate a 'net: \n",
    "\n",
    "1. Load Data\n",
    "2. Define Model\n",
    "3. Compile Model\n",
    "4. Fit Model\n",
    "5. Evaluate Model\n",
    "\n",
    "You saw these steps in our Keras Perceptron Sample, but let's walk thru each step in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md5D67XwqVAf",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "bn09phMBpY1J",
    "outputId": "1c45fb6a-e3cb-4ec1-fb85-de52b3c60bae"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Stretch - use dropout \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(28, 28)"
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(60000, 28, 28)"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Variable Types\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') /255.\n",
    "\n",
    "# Correct Encoding on Y\n",
    "# What softmax expects = [0,0,0,0,0,1,0,0,0,0]\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0xMqOyTs5xt"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bp9USczrfu6M"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAzHLg27thoN"
   },
   "source": [
    "I'll instantiate my model as a \"sequential\" model. This just means that I'm going to tell Keras what my model's architecture should be one layer at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSNsL49Xp6KI"
   },
   "outputs": [],
   "source": [
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCYX6QzJtvpG"
   },
   "source": [
    "Adding a \"Dense\" layer to our model is how we add \"vanilla\" perceptron-based layers to our neural network. These are also called \"fully-connected\" or \"densely-connected\" layers. They're used as a layer type in lots of other Neural Net Architectures but they're not referred to as perceptrons or multi-layer perceptrons very often in those situations even though that's what they are.\n",
    "\n",
    " > [\"Just your regular densely-connected NN layer.\"](https://keras.io/layers/core/)\n",
    " \n",
    " The first argument is how many neurons we want to have in that layer. To create a perceptron-esque model we will just set it to 10. Our architecture is just an input and output layer. We will tell it that there will be 784 inputs coming into this layer from our dataset and set it to use the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "GNzOLidxtvFa",
    "outputId": "35b1457d-0189-49f1-aa6d-3ef15b29bd6e"
   },
   "outputs": [],
   "source": [
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(10,activation=\"softmax\")) #Relu is valid option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnI3jwKMtBL2",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Compile Model\n",
    "Using binary_crossentropy as the loss function here is just telling keras that I'm doing binary classification so that it can use the appropriate loss function accordingly. If we were predicting non-binary categories we might assign something like `categorical_crossentropy`. We're also telling keras that we want it to report model accuracy as our main error metric for each epoch. We will also be able to see the overall accuracy once the model has finished training.\n",
    "\n",
    "#### Adam Optimizer\n",
    "Check out this links for more background on the Adam optimizer and Stohastic Gradient Descent\n",
    "* [Adam Optimization Algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "* [Adam Optimizer - original paper](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qp6xwYaqurRO"
   },
   "outputs": [],
   "source": [
    "# how the loss function is going to be used by the loss function, and what optimization function is going to be used to maximize\n",
    "# if catigorizing a binary fproblem binary_crossentropy is used\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_17\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_3 (Flatten)          (None, 784)               0         \n_________________________________________________________________\ndense_32 (Dense)             (None, 32)                25120     \n_________________________________________________________________\ndense_33 (Dense)             (None, 10)                330       \n=================================================================\nTotal params: 25,450\nTrainable params: 25,450\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dW8SZ2Ls9SX",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Fit Model\n",
    "\n",
    "Lets train it up! `model.fit()` has a `batch_size` parameter that we can use if we want to do mini-batch epochs, but since this tabular dataset is pretty small we're just going to delete that parameter. Keras' default `batch_size` is `None` so omiting it will tell Keras to do batch epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.3580 - accuracy: 0.8984\nEpoch 2/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1954 - accuracy: 0.9437\nEpoch 3/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1512 - accuracy: 0.9551\nEpoch 4/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1273 - accuracy: 0.9628\nEpoch 5/10\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.1075 - accuracy: 0.9679\nEpoch 6/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0970 - accuracy: 0.9703\nEpoch 7/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0868 - accuracy: 0.9734\nEpoch 8/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0791 - accuracy: 0.9757\nEpoch 9/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0722 - accuracy: 0.9776\nEpoch 10/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0661 - accuracy: 0.9797\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f84adc08a10>"
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "# if unspecifed the defualt batch_size is 32\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "313/313 [==============================] - 0s 2ms/step - loss: 0.1058 - accuracy: 0.9703\n\n\nValidation Data Metrics:\nloss: 0.10578204691410065\naccuracy: 97.0300018787384\n"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test,y_test)\n",
    "print(\"\\n\")\n",
    "print(\"Validation Data Metrics:\")\n",
    "print(f\"{model.metrics_names[0]}: {scores[0]}\")\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHYB7k9q3O8T"
   },
   "source": [
    "### Unstable Results\n",
    "\n",
    "You'll notice that if we rerun the results might differ from the origin run. This can be explain by a bunch of factors. Check out some of them in this article: \n",
    "\n",
    "<https://machinelearningmastery.com/randomness-in-machine-learning/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to leverage the Keras `Sequential` api to estimate a feed forward neural networks on a dataset.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('intro_nn': conda)",
   "language": "python",
   "name": "python37664bitintronncondabe27099557db4cd1a4d5547b016f4f28"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}